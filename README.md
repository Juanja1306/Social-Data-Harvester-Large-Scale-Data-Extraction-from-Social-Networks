# ğŸŒ Social Data Harvester: Large-Scale Data Extraction from Social Networks

[![Python 3.12+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Playwright](https://img.shields.io/badge/Playwright-1.57-green.svg)](https://playwright.dev/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.128-teal.svg)](https://fastapi.tiangolo.com/)

> **Herramienta de investigaciÃ³n**: Scraper multi-plataforma con interfaz web, almacenamiento SQLite y anÃ¡lisis de sentimientos con LLM (DeepSeek).

---

## ğŸ“‘ Ãndice

- [VisiÃ³n general](#-visiÃ³n-general)
- [CaracterÃ­sticas](#-caracterÃ­sticas)
- [Arquitectura](#-arquitectura)
- [Plataformas soportadas](#-plataformas-soportadas)
- [InstalaciÃ³n](#-instalaciÃ³n)
- [ConfiguraciÃ³n](#-configuraciÃ³n)
- [Uso](#-uso)
- [Estructura del proyecto](#-estructura-del-proyecto)
- [Bases de datos y pipeline de datos](#-bases-de-datos-y-pipeline-de-datos)
- [API REST](#-api-rest)
- [Interfaz web](#-interfaz-web)
- [AnÃ¡lisis LLM (DeepSeek)](#-anÃ¡lisis-llm-deepseek)
- [GrÃ¡ficas](#-grÃ¡ficas)
- [Comentarios y explicaciones](#-comentarios-y-explicaciones)
- [Detalles tÃ©cnicos](#-detalles-tÃ©cnicos)
- [SoluciÃ³n de problemas](#-soluciÃ³n-de-problemas)
- [Aspectos legales y Ã©ticos](#-aspectos-legales-y-Ã©ticos)
- [Referencias](#-referencias)

---

## ğŸ¯ VisiÃ³n general

**Social Data Harvester** es una aplicaciÃ³n web que permite extraer contenido pÃºblico de varias redes sociales en paralelo, guardar los resultados en SQLite y analizar sentimientos (positivo/negativo/neutral) por post y por comentario usando el modelo DeepSeek. Incluye reportes por red, grÃ¡ficas y una secciÃ³n para ver cada comentario con su explicaciÃ³n de sentimiento.

### Capacidades principales

- **Scraping multi-plataforma**: LinkedIn, Instagram, Facebook y Twitter en paralelo.
- **BÃºsqueda por frase exacta**: Las consultas se envÃ­an entre comillas dobles para coincidencia exacta en cada red.
- **Interfaz web**: FastAPI + frontend estÃ¡tico (HTML/CSS/JS) con log en tiempo real por WebSocket.
- **Almacenamiento SQLite**: `resultados.db` (datos crudos), `reportes.db` (reportes texto) y `analisis.db` (JSON por publicaciÃ³n).
- **AnÃ¡lisis de sentimientos**: DeepSeek analiza cada post y cada comentario; muestra progreso por red hasta que terminen todas.
- **Comentarios y explicaciones**: Vista por Request y red con texto del post/comentario y explicaciÃ³n por Ã­tem.
- **GrÃ¡ficas**: GeneraciÃ³n de grÃ¡ficas a partir de resultados y anÃ¡lisis (por Request).

---

## âœ¨ CaracterÃ­sticas

### Scraping

- Procesos independientes por red (multiprocessing).
- LÃ­mite configurable de posts por red.
- Sesiones con cookies; login manual si no hay cookies vÃ¡lidas.
- Delays aleatorios y comportamiento tipo humano para reducir detecciÃ³n.
- Parada ordenada de todos los procesos.

### Interfaz y datos

- Log de actividad en tiempo real (WebSocket).
- Selector de Request para descargar CSV o ejecutar anÃ¡lisis LLM.
- Descarga de resultados en CSV (todos o por Request).
- Reportes de anÃ¡lisis LLM por red (texto y JSON).
- GalerÃ­a de grÃ¡ficas por Request.

### AnÃ¡lisis LLM

- AnÃ¡lisis por red (LinkedIn, Instagram, Twitter, Facebook).
- Progreso visible: â€œCompletada [Red]â€ / â€œAnalizando [Red]â€¦â€ hasta que terminen todas.
- Sentimiento y explicaciÃ³n breve por post y por comentario.
- SecciÃ³n **Comentarios y explicaciones**: ver cada publicaciÃ³n con post, comentarios y explicaciÃ³n por Ã­tem.

### GrÃ¡ficas

- GeneraciÃ³n desde `resultados.db` y `analisis.db`.
- ImÃ¡genes guardadas en `images/<request>/`.
- VisualizaciÃ³n en carrusel en la web.

---

## ğŸ—ï¸ Arquitectura

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚           Navegador (Usuario)           â”‚
                    â”‚  index.html + app.js + style.css        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚ HTTP / WebSocket
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚         FastAPI (app/main.py)           â”‚
                    â”‚  /api/scrape/*, /api/llm/*, /api/charts  â”‚
                    â”‚  /api/comments-explained, /api/requests â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                              â”‚                              â”‚
        â–¼                              â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Multiproc.   â”‚            â”‚  Thread drain   â”‚            â”‚  SQLite         â”‚
â”‚  Scrapers     â”‚            â”‚  llm_queue      â”‚            â”‚  resultados.db  â”‚
â”‚  (process/)   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  (completados) â”‚            â”‚  reportes.db    â”‚
â”‚  + Writer     â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  analisis.db    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                                             â”‚
        â”‚ Playwright                                                  â”‚
        â–¼                                                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chromium     â”‚                                            â”‚  LLM DeepSeek   â”‚
â”‚  (por red)    â”‚                                            â”‚  (sentimiento   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚   por post/comm) â”‚
                                                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flujo

1. Usuario configura Request (tema), mÃ¡ximo de posts y redes en la web.
2. Backend arranca un proceso escritor (SQLite) y un proceso por cada red seleccionada.
3. Cada scraper usa Playwright/Chromium, hace bÃºsqueda (query entre comillas) y escribe en la cola de resultados.
4. El proceso escritor escribe en `resultados.db`.
5. AnÃ¡lisis LLM: el usuario elige Request y lanza el anÃ¡lisis; se ejecuta un proceso por red; cada uno escribe en `reportes.db` y `analisis.db`; el backend trackea â€œcompletada [red]â€ y la UI muestra progreso hasta que terminen todas.
6. GrÃ¡ficas: se generan desde las DB y se sirven desde `images/<request>/`.

---

## ğŸŒ Plataformas soportadas

| Plataforma   | Estado  | Uso por defecto | AutenticaciÃ³n        |
|-------------|--------|-----------------|----------------------|
| **LinkedIn**  | âœ… Activo | SÃ­              | Cookies + manual     |
| **Instagram** | âœ… Activo | SÃ­              | Cookies + manual     |
| **Facebook**  | âœ… Activo | SÃ­              | Cookies + manual     |
| **Twitter/X**  | âœ… Activo | SÃ­              | Cookies + manual     |
| **Reddit**    | âœ… CÃ³digo | No (no en UI por defecto) | Cookies + manual     |

---

## ğŸ“¦ InstalaciÃ³n

### Requisitos

- **Python**: 3.8 o superior
- **Sistema**: Windows, macOS o Linux
- **Chromium**: instalado vÃ­a Playwright

### Pasos

1. **Clonar el repositorio**

```bash
git clone https://github.com/Juanja1306/Social-Data-Harvester-Large-Scale-Data-Extraction-from-Social-Networks.git
cd Social-Data-Harvester-Large-Scale-Data-Extraction-from-Social-Networks
```

2. **Entorno virtual (recomendado)**

```bash
python -m venv venv
# Windows:
venv\Scripts\activate
# Linux/macOS:
source venv/bin/activate
```

3. **Dependencias**

```bash
pip install -r requirements.txt
```

4. **Playwright (Chromium)**

```bash
playwright install chromium
```

5. **Variables de entorno**

Crear un archivo `.env` en la raÃ­z del proyecto (ver [ConfiguraciÃ³n](#-configuraciÃ³n)).

---

## âš™ï¸ ConfiguraciÃ³n

### Archivo `.env`

En la raÃ­z del proyecto, crea `.env` con al menos:

```env
# Obligatorio para anÃ¡lisis LLM (DeepSeek)
DEEPSEEK_API_KEY=tu_api_key_de_deepseek
```

Opcionalmente puedes definir credenciales para las redes (el proyecto puede usarlas segÃºn la lÃ³gica de cada scraper); no incluyas datos reales en el README ni en el repositorio.

### ConfiguraciÃ³n en cÃ³digo

- **Redes por defecto**: `app/config.py` â†’ `DEFAULT_NETWORKS` (LinkedIn, Instagram, Facebook, Twitter).
- **Redes para LLM**: `LLM_NETWORKS` en el mismo archivo.
- **Bases de datos**: `DATABASE_FILENAME`, `REPORTES_DB_FILENAME`, `ANALISIS_DB_FILENAME` en `app/config.py`.
- **Timeout al parar procesos**: `STOP_JOIN_TIMEOUT` en `app/config.py`.

---

## ğŸš€ Uso

### Arrancar la aplicaciÃ³n

Desde la **raÃ­z del proyecto**:

```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

Abre en el navegador: `http://localhost:8000`

### Flujo bÃ¡sico

1. **ConfiguraciÃ³n de bÃºsqueda**
   - Request (tema): texto o selecciÃ³n del desplegable (basado en Requests ya usados).
   - MÃ¡ximo de posts por red.
   - Marcar las redes a usar (LinkedIn, Instagram, Facebook, Twitter).

2. **Iniciar bÃºsqueda**
   - Clic en **Iniciar bÃºsqueda**. Se abren ventanas de Chromium por red; si hace falta, inicia sesiÃ³n manualmente.
   - La bÃºsqueda se envÃ­a como **frase exacta** (entre comillas dobles) en cada plataforma.
   - El log se actualiza en tiempo real.

3. **Parar bÃºsqueda**
   - **Parar bÃºsqueda** detiene todos los procesos y persiste lo ya guardado en `resultados.db`.

4. **Resultados y anÃ¡lisis**
   - **Descargar CSV**: elegir Request (o â€œTodosâ€) y usar el enlace de descarga.
   - **AnÃ¡lisis LLM**: elegir Request y pulsar **Ejecutar anÃ¡lisis LLM**. VerÃ¡s â€œCompletada [Red]â€ / â€œAnalizando [Red]â€¦â€ hasta que terminen todas las redes; luego se muestran las pestaÃ±as de reportes.
   - **Comentarios y explicaciones**: elegir Request y opcionalmente Red, luego **Ver comentarios y explicaciones** para ver cada post/comentario con su explicaciÃ³n.
   - **GrÃ¡ficas**: elegir Request y **Generar grÃ¡ficas**; se muestran en la galerÃ­a inferior.

---

## ğŸ“ Estructura del proyecto

```
Social-Data-Harvester--Large-Scale-Data-Extraction-from-Social-Networks/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # FastAPI: rutas, WebSocket, estado
â”‚   â”œâ”€â”€ config.py            # DB, redes, timeouts
â”‚   â”œâ”€â”€ scraping.py           # run_scraper, run_llm_process, SQLite writer, export CSV
â”‚   â”œâ”€â”€ charts.py            # GrÃ¡ficas desde resultados.db y analisis.db
â”‚   â””â”€â”€ static/
â”‚       â”œâ”€â”€ index.html       # Interfaz web
â”‚       â”œâ”€â”€ css/style.css
â”‚       â””â”€â”€ js/app.js
â”‚
â”œâ”€â”€ process/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ Process_Linkedin.py
â”‚   â”œâ”€â”€ Process_Instagram.py
â”‚   â”œâ”€â”€ Process_Facebook.py
â”‚   â”œâ”€â”€ Process_Twitter.py
â”‚   â””â”€â”€ Process_Reddit.py
â”‚
â”œâ”€â”€ LLM/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ sentiment_analyzer_deepseek.py   # AnÃ¡lisis sentimiento (DeepSeek)
â”‚
â”œâ”€â”€ .env                     # DEEPSEEK_API_KEY (y opc. credenciales)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ resultados.db             # Generado: datos scrape (RedSocial, Request, Data, etc.)
â”œâ”€â”€ reportes.db              # Generado: reportes texto por red/request
â”œâ”€â”€ analisis.db              # Generado: JSON de anÃ¡lisis por publicaciÃ³n/red/request
â””â”€â”€ images/                  # Generado: grÃ¡ficas por request
    â””â”€â”€ <request>/
        â””â”€â”€ *.png
```

---

## ğŸ—„ï¸ Bases de datos y pipeline de datos

### `resultados.db` â€” Tabla `resultados`

| Columna          | Tipo   | DescripciÃ³n                          |
|------------------|--------|--------------------------------------|
| id               | INTEGER| PK autoincremental                   |
| RedSocial        | TEXT   | LinkedIn, Instagram, Facebook, Twitter |
| IDP              | INTEGER| ID de proceso                        |
| Request          | TEXT   | Tema de bÃºsqueda                     |
| FechaPeticion    | TEXT   | Fecha/hora de la peticiÃ³n            |
| FechaPublicacion | TEXT   | Fecha de la publicaciÃ³n (si existe)  |
| idPublicacion    | TEXT   | Identificador de la publicaciÃ³n      |
| Data             | TEXT   | `post|comentario1|comentario2|...`   |

### `reportes.db` â€” Tabla `reportes`

Reportes de texto por red y request (estadÃ­sticas, mÃ©tricas de anÃ¡lisis LLM).

| Columna   | Tipo | DescripciÃ³n        |
|-----------|-----|--------------------|
| id        | INTEGER | PK              |
| network   | TEXT   | Red social     |
| request   | TEXT   | Request        |
| content   | TEXT   | Reporte texto  |
| created_at| TEXT   | Fecha creaciÃ³n |

### `analisis.db` â€” Tabla `analisis`

JSON con anÃ¡lisis por publicaciÃ³n: sentimiento y explicaciÃ³n del post y de cada comentario.

| Columna    | Tipo   | DescripciÃ³n                          |
|------------|--------|--------------------------------------|
| id         | INTEGER| PK                                   |
| network    | TEXT   | Red social                           |
| request    | TEXT   | Request                              |
| content_json| TEXT  | Lista de objetos por publicaciÃ³n    |
| created_at | TEXT   | Fecha creaciÃ³n                       |

Cada elemento de `content_json` incluye `idPublicacion`, `analisis_post` (sentimiento, explicaciÃ³n), `analisis_comentarios` (lista de sentimiento y explicaciÃ³n por comentario).

---

## ğŸ”Œ API REST

Base: `/api`

| MÃ©todo | Ruta | DescripciÃ³n |
|--------|------|-------------|
| POST | `/scrape/start` | Inicia scraping (body: query, max_posts, networks) |
| POST | `/scrape/stop` | Detiene scraping |
| GET | `/scrape/status` | Estado: running, networks, llm_running, llm_networks, llm_completed_networks |
| GET | `/requests` | Lista de Requests distintos (para selectores) |
| GET | `/results` | CSV de resultados (?request= opcional) |
| GET | `/comments-explained` | Publicaciones con post/comentarios y explicaciÃ³n (?request=, &network= opcional) |
| POST | `/llm/analyze` | Lanza anÃ¡lisis LLM (body: request, networks) |
| GET | `/llm/reports` | Lista de reportes por red (has_text, has_json) |
| GET | `/llm/reports/{network}` | Contenido del reporte (?format=text|json, &request= opcional) |
| POST | `/charts/generate` | Genera grÃ¡ficas (body: request opcional) |
| GET | `/charts/image/{folder}/{filename}` | Sirve imagen de grÃ¡fica |

WebSocket: `/ws/log` â€” Log de actividad en tiempo real.

---

## ğŸ–¥ï¸ Interfaz web

- **ConfiguraciÃ³n de bÃºsqueda**: Request, mÃ¡ximo de posts, checkboxes de redes.
- **Log de actividad**: Mensajes en vivo (WebSocket); opciÃ³n autoScroll.
- **Resultados y anÃ¡lisis**: Selector de Request para descargar CSV, botÃ³n de anÃ¡lisis LLM, selector para generar grÃ¡ficas.
- **Reportes de anÃ¡lisis LLM**: PestaÃ±as por red; contenido de reporte (texto) o JSON segÃºn formato.
- **Comentarios y explicaciones**: Selector de Request y Red; lista de publicaciones con post, comentarios y explicaciÃ³n por Ã­tem.
- **GalerÃ­a de grÃ¡ficas**: Carrusel de imÃ¡genes generadas por Request.

---

## ğŸ¤– AnÃ¡lisis LLM (DeepSeek)

- **Modelo**: DeepSeek vÃ­a API (cliente compatible con OpenAI).
- **Entrada**: CSV exportado por Request desde `resultados.db` (una fila por publicaciÃ³n; columna `Data` = post\|comentarios).
- **Proceso**: Por cada red seleccionada se lanza un proceso que analiza cada post y cada comentario; devuelve sentimiento (Positivo/Negativo/Neutral) y explicaciÃ³n breve.
- **Salida**: Se guarda en `reportes.db` (texto) y `analisis.db` (JSON por publicaciÃ³n).
- **UI**: Durante la ejecuciÃ³n se muestra â€œCompletada [Red]â€ o â€œAnalizando [Red]â€¦â€ por cada red; el panel de carga solo se oculta cuando **todas** han terminado.

---

## ğŸ“Š GrÃ¡ficas

- **Origen**: `resultados.db` y `analisis.db` (conteos por red, por sentimiento, fechas, etc.).
- **GeneraciÃ³n**: `app/charts.py`; imÃ¡genes en `images/<request>/`.
- **VisualizaciÃ³n**: En la web, secciÃ³n â€œGalerÃ­a de grÃ¡ficasâ€ con carrusel por Request.

---

## ğŸ’¬ Comentarios y explicaciones

- **Origen**: Cruce de `resultados.db` (texto post/comentarios) y `analisis.db` (sentimiento y explicaciÃ³n por Ã­tem).
- **Uso**: En la web, secciÃ³n â€œComentarios y explicacionesâ€: elegir Request y opcionalmente Red; al pulsar **Ver comentarios y explicaciones** se listan las publicaciones con:
  - Post: texto, sentimiento, explicaciÃ³n.
  - Comentarios: texto, sentimiento y explicaciÃ³n por comentario.
- **API**: `GET /api/comments-explained?request=...&network=...` (network opcional).

---

## ğŸ”§ Detalles tÃ©cnicos

- **Multiprocessing**: Un proceso por red de scraping + un proceso escritor; colas `Queue` y `Event` para parada.
- **LLM**: Un proceso por red; cada uno escribe en una cola al terminar; un hilo en el proceso principal drena la cola y actualiza `llm_completed_networks` para el progreso en la UI.
- **BÃºsqueda**: En cada scraper la query se envÃ­a entre comillas dobles en la URL/parÃ¡metros para bÃºsqueda por frase exacta.
- **Cookies**: Los scrapers pueden guardar/cargar cookies por plataforma para reutilizar sesiÃ³n.

---

## ğŸ› SoluciÃ³n de problemas

### Playwright / Chromium

```bash
playwright install chromium
```

### â€œNo hay datosâ€ / â€œNo resultsâ€

- AsegÃºrate de haber ejecutado al menos una bÃºsqueda y de que `resultados.db` existe en la raÃ­z del proyecto.
- Para anÃ¡lisis LLM o comentarios, comprueba que el Request elegido tenga filas en `resultados.db`.

### AnÃ¡lisis LLM no arranca o falla

- Verifica que `.env` tenga `DEEPSEEK_API_KEY` vÃ¡lida.
- Revisa que el Request tenga datos en el CSV (exportaciÃ³n desde `resultados.db`).

### Scraper bloqueado o sin progreso

- Algunos scrapers tienen detecciÃ³n de estancamiento y lÃ­mite de iteraciones; revisa el log en la UI.
- Si pide login, inicia sesiÃ³n manualmente en la ventana de Chromium que se abre.

### GrÃ¡ficas vacÃ­as

- Genera primero el anÃ¡lisis LLM para el Request deseado; muchas grÃ¡ficas dependen de `analisis.db`.

---

## âš–ï¸ Aspectos legales y Ã©ticos

- Herramienta orientada a **investigaciÃ³n y uso educativo**.
- Usar solo sobre contenido **pÃºblico** y respetando los tÃ©rminos de uso de cada plataforma.
- No usar para fines comerciales no autorizados, scraping de contenido privado, acoso ni reventa de datos.
- Recomendable: respetar robots.txt, limitar frecuencia de peticiones y anonimizar datos personales en publicaciones.

---

## ğŸ“š Referencias

- [Playwright para Python](https://playwright.dev/python/)
- [FastAPI](https://fastapi.tiangolo.com/)
- [DeepSeek API](https://platform.deepseek.com/)
- [Multiprocessing en Python](https://docs.python.org/3/library/multiprocessing.html)
